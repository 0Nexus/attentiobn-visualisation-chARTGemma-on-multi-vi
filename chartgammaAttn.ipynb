{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf74402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD UTILS.PY \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import (\n",
    "    load_image,\n",
    "    aggregate_llm_attention, aggregate_vit_attention,\n",
    "    heterogenous_stack,\n",
    "    show_mask_on_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_path, model_base=None, model_name=\"chartgemma\", load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Load ChartGemma model (based on PaliGemma architecture)\n",
    "\n",
    "    Args:\n",
    "        model_path: Path or HuggingFace model ID (e.g., \"ahmed-masry/chartgemma\")\n",
    "        model_base: Base model path (for LoRA, not typically used with ChartGemma)\n",
    "        model_name: Model name identifier\n",
    "        load_8bit: Load model in 8-bit quantization\n",
    "        load_4bit: Load model in 4-bit quantization\n",
    "        device_map: Device mapping strategy\n",
    "        device: Target device\n",
    "        use_flash_attn: Use flash attention (if supported)\n",
    "\n",
    "    Returns:\n",
    "        processor, model, context_len\n",
    "    \"\"\"\n",
    "    kwargs = {\"device_map\": device_map, **kwargs}\n",
    "\n",
    "    if device != \"cuda\":\n",
    "        kwargs['device_map'] = {\"\": device}\n",
    "\n",
    "    # Configure quantization\n",
    "    if load_8bit:\n",
    "        kwargs['load_in_8bit'] = True\n",
    "    elif load_4bit:\n",
    "        kwargs['load_in_4bit'] = True\n",
    "        kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "\n",
    "    # Flash attention support for PaliGemma\n",
    "    if use_flash_attn:\n",
    "        kwargs['attn_implementation'] = 'flash_attention_2'\n",
    "    kwargs['attn_implementation'] = 'eager'\n",
    "\n",
    "    print(f'Loading ChartGemma model from {model_path}...')\n",
    "\n",
    "    # Load processor (combines tokenizer and image processor for PaliGemma)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "    # Load model\n",
    "    if model_base is not None:\n",
    "        # Handle LoRA or fine-tuned models\n",
    "        from peft import PeftModel\n",
    "        print(f'Loading base model from {model_base}...')\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_base,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        print(f\"Loading adapter weights from {model_path}\")\n",
    "        model = PeftModel.from_pretrained(model, model_path)\n",
    "        print(f\"Merging weights...\")\n",
    "        model = model.merge_and_unload()\n",
    "        print('Model loaded and merged successfully')\n",
    "    else:\n",
    "        # Load full model directly\n",
    "        model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs\n",
    "        )\n",
    "        print('Model loaded successfully')\n",
    "\n",
    "    # Get context length\n",
    "    if hasattr(model.config, \"max_position_embeddings\"):\n",
    "        context_len = model.config.max_position_embeddings\n",
    "    elif hasattr(model.config, \"text_config\") and hasattr(model.config.text_config, \"max_position_embeddings\"):\n",
    "        context_len = model.config.text_config.max_position_embeddings\n",
    "    else:\n",
    "        context_len = 2048  # Default fallback\n",
    "\n",
    "    return processor, model, context_len\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from questions import mini_VLAT_questions\n",
    "import os\n",
    "\n",
    "model_path = \"ahmed-masry/chartgemma\"\n",
    "\n",
    "# Load the model\n",
    "load_8bit = False\n",
    "load_4bit = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor, model, context_len = load_pretrained_model(\n",
    "    model_path,\n",
    "    None,  # model_base\n",
    "    \"chartgemma\",\n",
    "    load_8bit,\n",
    "    load_4bit,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get the number of image tokens (patches)\n",
    "vision_model = model.vision_tower if hasattr(model, 'vision_tower') else model.vision_model\n",
    "if hasattr(vision_model.config, 'num_image_tokens'):\n",
    "    num_image_tokens = vision_model.config.num_image_tokens\n",
    "else:\n",
    "    image_size = vision_model.config.image_size\n",
    "    patch_size = vision_model.config.patch_size\n",
    "    num_image_tokens = (image_size // patch_size) ** 2\n",
    "\n",
    "# Calculate grid size for attention visualization\n",
    "grid_size = int(np.sqrt(num_image_tokens))\n",
    "\n",
    "# Create output directory for visualizations\n",
    "os.makedirs(\"attention_outputs\", exist_ok=True)\n",
    "\n",
    "# Process each question\n",
    "for idx, question_data in enumerate(mini_VLAT_questions):\n",
    "    chart_type = question_data[0]\n",
    "    prompt_text = question_data[1]\n",
    "    image_path = question_data[2]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {idx + 1}/{len(mini_VLAT_questions)}: {chart_type}\")\n",
    "    print(f\"Question: {prompt_text}\")\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Load image\n",
    "    try:\n",
    "        image = load_image(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(text=prompt_text, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response with attention\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=True,\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = processor.decode(outputs[\"sequences\"][0], skip_special_tokens=True).strip()\n",
    "    print(f\"\\nResponse: {response}\\n\")\n",
    "    \n",
    "    # Construct attention matrix\n",
    "    aggregated_prompt_attention = []\n",
    "    for i, layer in enumerate(outputs[\"attentions\"][0]):\n",
    "        layer_attns = layer.squeeze(0)\n",
    "        attns_per_head = layer_attns.mean(dim=0)\n",
    "        cur = attns_per_head[:-1].cpu().clone()\n",
    "        cur[1:, 0] = 0.\n",
    "        cur[1:] = cur[1:] / cur[1:].sum(-1, keepdim=True)\n",
    "        aggregated_prompt_attention.append(cur)\n",
    "    aggregated_prompt_attention = torch.stack(aggregated_prompt_attention).mean(dim=0)\n",
    "    \n",
    "    # Build LLM attention matrix\n",
    "    llm_attn_matrix = heterogenous_stack(\n",
    "        [torch.tensor([1])]\n",
    "        + list(aggregated_prompt_attention)\n",
    "        + list(map(aggregate_llm_attention, outputs[\"attentions\"]))\n",
    "    )\n",
    "    \n",
    "    # Calculate token positions\n",
    "    total_sequence_len = outputs[\"sequences\"].shape[1]\n",
    "    input_prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    vision_token_start = 0\n",
    "    vision_token_end = num_image_tokens\n",
    "    \n",
    "    output_token_start = input_prompt_len\n",
    "    output_token_len = total_sequence_len - input_prompt_len\n",
    "    output_token_end = total_sequence_len\n",
    "    \n",
    "    # Validate\n",
    "    if output_token_len <= 0:\n",
    "        print(f\"Warning: No new tokens generated for question {idx + 1}\")\n",
    "        continue\n",
    "    \n",
    "    # Create visualization\n",
    "    num_image_per_row = 8\n",
    "    image_ratio = image.size[0] / image.size[1]\n",
    "    num_rows = output_token_len // num_image_per_row + (1 if output_token_len % num_image_per_row != 0 else 0)\n",
    "    \n",
    "    if num_rows == 0:\n",
    "        num_rows = 1\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        num_rows, num_image_per_row,\n",
    "        figsize=(10, (10 / num_image_per_row) * image_ratio * num_rows),\n",
    "        dpi=150\n",
    "    )\n",
    "    \n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.2)\n",
    "    \n",
    "    vis_overlayed_with_attn = True\n",
    "    output_token_inds = list(range(output_token_start, output_token_end))\n",
    "    vision_model = model.vision_tower if hasattr(model, 'vision_tower') else model.vision_model\n",
    "\n",
    "# For PaliGemma, we need to get the vision attention from the last forward pass\n",
    "# The attention is stored differently than in LLaVA\n",
    "# You may need to run a forward pass with output_attentions=True first\n",
    "\n",
    "# Get vision attention - PaliGemma structure\n",
    "    if hasattr(vision_model, 'image_attentions'):\n",
    "        vision_attentions = vision_model.image_attentions\n",
    "    else:\n",
    "        # If not stored, you'll need to do a forward pass on just the vision model\n",
    "        # with the image to get the attentions\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = vision_model(\n",
    "                pixel_values=inputs[\"pixel_values\"],\n",
    "                output_attentions=True\n",
    "            )\n",
    "            vision_attentions = vision_outputs.attentions\n",
    "\n",
    "    # Aggregate vision attention\n",
    "    # PaliGemma typically has 27 vision layers\n",
    "    select_layer = -1  # Use last layer, or specify which layer you want\n",
    "    all_prev_layers = True\n",
    "\n",
    "    if all_prev_layers and select_layer < 0:\n",
    "        # Average all layers\n",
    "        stacked_attentions = torch.stack([attn.mean(dim=1).squeeze(0) for attn in vision_attentions])\n",
    "        vis_attn_matrix = stacked_attentions.mean(dim=0)\n",
    "    else:\n",
    "        # Use specific layer\n",
    "        layer_idx = select_layer if select_layer >= 0 else len(vision_attentions) + select_layer\n",
    "        vis_attn_matrix = vision_attentions[layer_idx].mean(dim=1).squeeze(0)\n",
    "\n",
    "    # Remove CLS token attention if present (first token)\n",
    "    if vis_attn_matrix.shape[0] > num_image_tokens:\n",
    "        vis_attn_matrix = vis_attn_matrix[1:, 1:]  # Remove CLS token\n",
    "    else:\n",
    "        vis_attn_matrix = vis_attn_matrix\n",
    "    patch_size = vision_model.config.patch_size\n",
    "    image_size_model = vision_model.config.image_size\n",
    "    grid_size = image_size_model // patch_size\n",
    "\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i >= output_token_len:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        target_token_ind = output_token_inds[i]\n",
    "        attn_weights_over_vis_tokens = llm_attn_matrix[target_token_ind][vision_token_start:vision_token_end]\n",
    "        \n",
    "        if len(attn_weights_over_vis_tokens) == 0 or attn_weights_over_vis_tokens.sum() == 0:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        \n",
    "        attn_weights_over_vis_tokens = attn_weights_over_vis_tokens / attn_weights_over_vis_tokens.sum()\n",
    "        \n",
    "        attn_over_image = []\n",
    "        for weight, vis_attn in zip(attn_weights_over_vis_tokens, vis_attn_matrix):\n",
    "            vis_attn = vis_attn.reshape(grid_size, grid_size)\n",
    "            attn_over_image.append(vis_attn * weight)\n",
    "        attn_over_image = torch.stack(attn_over_image).sum(dim=0)\n",
    "        attn_over_image = attn_over_image / attn_over_image.max()\n",
    "        \n",
    "        attn_over_image = F.interpolate(\n",
    "            attn_over_image.unsqueeze(0).unsqueeze(0),\n",
    "            size=(image.size[1], image.size[0]),\n",
    "            mode='nearest',\n",
    "        ).squeeze()\n",
    "        \n",
    "        np_img = np.array(image)[:, :, ::-1]\n",
    "        img_with_attn, heatmap = show_mask_on_image(np_img, attn_over_image.cpu().numpy())\n",
    "        ax.imshow(heatmap if not vis_overlayed_with_attn else img_with_attn)\n",
    "        \n",
    "        token_id = outputs[\"sequences\"][0][target_token_ind]\n",
    "        ax.set_title(\n",
    "            processor.decode([token_id], skip_special_tokens=False).strip(),\n",
    "            fontsize=7,\n",
    "            pad=1\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Save visualization\n",
    "    output_filename = f\"attention_outputs/{chart_type}_{idx+1}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved visualization to: {output_filename}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completed processing {len(mini_VLAT_questions)} questions\")\n",
    "print(f\"Visualizations saved in: attention_outputs/\")\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
